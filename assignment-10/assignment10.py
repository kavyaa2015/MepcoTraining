# -*- coding: utf-8 -*-
"""ASSIGNMENT10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13tkLf31swhP6hNNnvpwXn77SvJelbKW1
"""

# Install dependencies
!pip install -U transformers datasets evaluate accelerate

import os
import numpy as np
from datasets import load_dataset
from transformers import (
    DistilBertTokenizerFast,
    DistilBertForSequenceClassification,
    TrainingArguments,
    Trainer,
)
import evaluate

# 1. Load Dataset
dataset = load_dataset("imdb")
print(dataset)

# 2. Load tokenizer and model
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

def tokenize_function(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=256)

tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset = tokenized_dataset.remove_columns(["text"])
tokenized_dataset = tokenized_dataset.rename_column("label", "labels")
tokenized_dataset.set_format("torch")

print("\nTokenization complete.")

# 3. Load pre-trained model
model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=2
)

# 4. Define Metrics
accuracy = evaluate.load("accuracy")

def compute_metrics(eval_preds):
    logits, labels = eval_preds
    preds = np.argmax(logits, axis=-1)
    return accuracy.compute(predictions=preds, references=labels)

# 5. Training arguments
from transformers import Seq2SeqTrainingArguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./bart-cnn-results",
    eval_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=2,
    predict_with_generate=True,
    weight_decay=0.01,
    fp16=True,
    save_total_limit=2,
    report_to="none"
)

# 6. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

print("\nStarting DistilBERT training on IMDb...")
trainer.train()
print("\nTraining completed.")

trainer.save_model("./distilbert-imdb-model")

# 7. Test model with pipeline
from transformers import pipeline

classifier = pipeline("sentiment-analysis", model="./distilbert-imdb-model", tokenizer=tokenizer)

test_examples = [
    "I really hated this movie. It was boring and too long.",
    "One of the best films I’ve seen this year. Beautiful cinematography!",
    "The plot made no sense but the action scenes were fun.",
    "Terrible acting, poor script, and overall a waste of time.",
    "A surprisingly good movie! I wasn’t expecting to enjoy it so much.",
    "Not bad, but not great either. Just average.",
]

print("\nTest Examples:")
for text in test_examples:
    result = classifier(text)
    print(f"\nText: {text}\nPrediction: {result}")

    # OUTPUT:
    #Training completed.
    #Device set to use cuda:0
    #Test Examples:
    #Text: I really hated this movie. It was boring and too long.
    #Prediction: [{'label': 'LABEL_0', 'score': 0.9998160004615784}]
    #Text: One of the best films I’ve seen this year. Beautiful cinematography!
    #Prediction: [{'label': 'LABEL_1', 'score': 0.9998059868812561}]
    #Text: The plot made no sense but the action scenes were fun.
    #Prediction: [{'label': 'LABEL_0', 'score': 0.9994381070137024}]
    #Text: Terrible acting, poor script, and overall a waste of time.
    #Prediction: [{'label': 'LABEL_0', 'score': 0.9998213648796082}]
    #Text: A surprisingly good movie! I wasn’t expecting to enjoy it so much.
    #Prediction: [{'label': 'LABEL_1', 'score': 0.9998030066490173}]
    #Text: Not bad, but not great either. Just average.
    #Prediction: [{'label': 'LABEL_0', 'score': 0.9987236857414246}]