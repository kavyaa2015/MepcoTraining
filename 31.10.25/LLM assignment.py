# -*- coding: utf-8 -*-
"""assignment_LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KS8We_bVvQeKB3sXEZONR4W5spDRjTQ2

1) Pretraining  

            a. Using distilgpt for textgeneration

            b. Chat generation using mistralai/Mistral-7B-Instruct-v0.2

            c. Summarize text using T5-small
"""

#a. Using distilgpt for textgeneration
from transformers import pipeline
generator = pipeline("text-generation", model="distilgpt2")
result = generator(
    "Once in a while, engineers need to recalibrate their instruments to maintain accuracy.",
    max_new_tokens=50,
    temperature=0.7,
    top_p=0.9,
    top_k=50,
    repetition_penalty=1.2
)
print(result[0]['generated_text'])

# b. Chat generation using mistralai/Mistral-7B-Instruct-v0.2
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model = "mistralai/Mistral-7B-Instruct-v0.2"
generator = pipeline("text-generation", model=model, torch_dtype="auto", device_map="auto")

prompt = "User: Explain what causes dopamine release.\nAssistant:"
response = generator(prompt, max_new_tokens=100, temperature=0.7)
print(response[0]['generated_text'])

#c. Summarize text using T5-small
from transformers import pipeline
summarizer = pipeline("summarization", model="t5-small")

text = """
Climate change is one of the biggest challenges facing humanity today. Rising global temperatures,
melting ice caps, and extreme weather events are disrupting ecosystems and communities worldwide.
Governments and organizations are working together to reduce carbon emissions, promote renewable
energy, and develop sustainable technologies. However, significant efforts are still needed to slow
down global warming and protect future generations from its devastating impacts.
"""

summary = summarizer(text, max_length=50, min_length=25, do_sample=False)
print(summary[0]['summary_text'])

import kagglehub
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW

# LOAD KAGGLE CSV
path = kagglehub.dataset_download("lakshmi25npathi/imdb-dataset-of-50k-movie-reviews")
csv_path = path + "/IMDB Dataset.csv"

df = pd.read_csv(csv_path)
df["label"] = df["sentiment"].map({"positive": 1, "negative": 0})
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

class IMDBDataset(Dataset):
    def __init__(self, df):
        self.texts = df["review"].tolist()
        self.labels = df["label"].tolist()

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        enc = tokenizer(
            self.texts[idx],
            truncation=True,
            padding="max_length",
            max_length=256,
            return_tensors="pt"
        )
        item = {k: v.squeeze(0) for k, v in enc.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

train_df = df.sample(frac=0.8, random_state=42)
test_df  = df.drop(train_df.index)

train_loader = DataLoader(IMDBDataset(train_df), batch_size=8, shuffle=True)
test_loader  = DataLoader(IMDBDataset(test_df), batch_size=8)

# LOAD DISTILBERT

device = "cuda" if torch.cuda.is_available() else "cpu"

model = AutoModelForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=2
).to(device)

optimizer = AdamW(model.parameters(), lr=3e-5)

#  TRAIN
model.train()
for epoch in range(1):
    for step, batch in enumerate(train_loader):
        batch = {k: v.to(device) for k, v in batch.items()}
        out = model(**batch)

        loss = out.loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if step % 200 == 0:
            print(f"Step {step} | Loss {loss.item():.4f}")

print("Training finished.")

# EXAMPLE SENTIMENT CHECK
model.eval()

def predict_sentiment(text):
    enc = tokenizer(
        text,
        truncation=True,
        padding="max_length",
        max_length=256,
        return_tensors="pt"
    ).to(device)

    with torch.no_grad():
        outputs = model(**enc)
        logits = outputs.logits
        predicted = torch.argmax(logits, dim=1).item()

    sentiment = "Positive" if predicted == 1 else "Negative"
    return sentiment

# Default example checks
sample1 = "This movie was absolutely wonderful. I loved every moment!"
sample2 = "This movie was not bad."

print("Sample 1:", predict_sentiment(sample1))
print("Sample 2:", predict_sentiment(sample2))

!pip install transformers -q

from transformers import pipeline

# Load SQuAD fine-tuned QA model
qa = pipeline("question-answering", model="deepset/roberta-base-squad2")

print("QA Chatbot ready! Type 'exit' to quit.\n")

context = """
The Eiffel Tower, constructed in 1889 for the Paris World's Fair, is one of the
most famous landmarks in the world. Designed by engineer Gustave Eiffel, the
tower stands 324 meters tall and was originally criticized by many artists and
intellectuals. Today, it attracts millions of visitors every year and serves as
a global symbol of France and Paris.


"""

while True:
    question = input("You: ")

    if question.lower() == "exit":
        print("Bot: Goodbye!")
        break

    result = qa({"question": question, "context": context})
    print("Bot:", result["answer"])

!pip install transformers sentencepiece accelerate --quiet

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def ask(prompt):
    inputs = tokenizer(prompt, return_tensors="pt")
    output = model.generate(**inputs, max_new_tokens=80)
    return tokenizer.decode(output[0], skip_special_tokens=True)


# Contexts & Questions
context_1 = """
The Nile River is the longest river in Africa. It flows through 11 countries
and is a major source of water for agriculture and transportation.
"""

question_1 = "How many countries does the Nile River flow through?"

context_2 = """
COVID-19 is a respiratory disease caused by the SARS-CoV-2 virus.
It first appeared in late 2019 and led to a global pandemic. Vaccines
were introduced in 2021 to reduce severe infections.
"""

question_2 = "When were COVID-19 vaccines introduced?"

# 1. Zero-shot prompting
print("ZERO-SHOT PROMPT")
prompt = f"""
Answer the question based only on the context.

Context: {context_1}
Question: {question_1}
"""
answer = ask(prompt)
print("Question:", question_1)
print("Answer:", answer)
# 2. One-shot prompting
print("ONE-SHOT PROMPT")
example = """
Context: The Sun is a star at the center of the solar system.
Question: What is the Sun?
Answer: A star at the center of the solar system.
"""

prompt = f"""
Use the example to answer the new question.

Example:
{example}

Now answer:
Context: {context_2}
Question: {question_2}
"""
answer = ask(prompt)
print("Question:", question_2)
print("Answer:", answer)

# 3. Few-shot prompting
print("FEW-SHOT PROMPT")
examples = """
Context: Water boils at 100°C at sea level.
Question: When does water boil?
Answer: At 100°C.

Context: The Great Wall of China is over 13,000 miles long.
Question: How long is the Great Wall of China?
Answer: Over 13,000 miles.
"""

prompt = f"""
Answer the question using the examples as reference.

Examples:
{examples}

Now answer:
Context: {context_1}
Question: {question_1}
"""
answer = ask(prompt)
print("Question:", question_1)
print("Answer:", answer)

# 4. Chain-of-thought prompting
print("CHAIN-OF-THOUGHT PROMPT")
prompt = f"""
Think step-by-step and then answer.

Context: {context_2}
Question: {question_2}
"""
answer = ask(prompt)
print("Question:", question_2)
print("Answer:", answer)

# 5. Role prompting
print("ROLE PROMPT")
prompt = f"""
You are a historian. Provide a clear answer.

Context: {context_1}
Question: {question_1}
"""
answer = ask(prompt)
print("Question:", question_1)
print("Answer:", answer)

# 6. Extractive QA prompting
print("EXTRACTIVE QA PROMPT")
prompt = f"""
Extract the answer exactly from the context. Use only the words found in the text.

Context: {context_2}
Question: {question_2}
"""
answer = ask(prompt)
print("Question:", question_2)
print("Answer:", answer)

# Install required libraries
!pip install transformers datasets sentencepiece sumy nltk --quiet

# Download required NLTK models
import nltk
nltk.download("punkt")
nltk.download("punkt_tab")

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer

# Input text
text = """
Artificial intelligence (AI) has rapidly transformed multiple industries.
From healthcare and finance to transportation and education, AI systems are
being used to assist professionals, automate repetitive tasks, and offer
data-driven insights. AI models such as neural networks learn patterns
from large datasets, allowing them to perform tasks such as image recognition,
language translation, and decision-making. Despite the advantages, concerns
exist regarding job displacement, privacy issues, and bias in AI systems.
Researchers and policymakers are working to ensure that AI development
remains safe, transparent, and beneficial to society.
"""

# ----------------------------
# 1. Extractive Summarization (TextRank)
# ----------------------------
def textrank_summary(text, sentence_count=3):
    parser = PlaintextParser.from_string(text, Tokenizer("english"))
    summarizer = TextRankSummarizer()
    summary_sentences = summarizer(parser.document, sentence_count)
    return " ".join(str(sentence) for sentence in summary_sentences)

extractive = textrank_summary(text)


# ----------------------------
# 2. Abstractive Summarization (Hugging Face FLAN-T5)
# ----------------------------
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "facebook/bart-large-cnn"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

prompt = "Summarize the following text in 3-4 sentences: " + text
inputs = tokenizer(prompt, return_tensors="pt", truncation=True)

summary_ids = model.generate(
    inputs.input_ids,
    num_beams=4,
    max_new_tokens=150,
    min_length=50
)

abstractive = tokenizer.decode(summary_ids[0], skip_special_tokens=True)


# ----------------------------
# Comparison Output
# ----------------------------
print("Original Text Length:", len(text.split()), "words\n")

print("EXTRACTIVE SUMMARY (TextRank):")
print(extractive)
print("\nLength:", len(extractive.split()), "words\n")

print("ABSTRACTIVE SUMMARY (FLAN-T5):")
print(abstractive)
print("\nLength:", len(abstractive.split()), "words\n")

from transformers import pipeline

# Retrieval-based
retrieval_classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
responses = {
    "technology": "New technologies like AI and blockchain are changing the world rapidly.",
    "greeting": "Hello! Nice to meet you. How can I help you today?",
    "wellbeing": "I'm just a bot, but I'm feeling great! How about you?"
}
candidate_labels = list(responses.keys())

def retrieval_response(user_input):
    result = retrieval_classifier(user_input, candidate_labels)
    best_label = result['labels'][0]
    return responses[best_label]

# Generative (GPT-2)
generative_chatbot = pipeline("text-generation", model="gpt2")

def generative_response(user_input):
    prompt = f"User: {user_input}\nChatbot:"
    response = generative_chatbot(prompt, max_length=50, do_sample=True, temperature=0.6)
    text = response[0]['generated_text'][len(prompt):].strip()
    # Take only the first line to avoid rambling
    return text.split("\n")[0]

# Single example input
example_input = "Can you tell me something about technology?"

retrieval_reply = retrieval_response(example_input)
generative_reply = generative_response(example_input)

print("===================================")
print(f"User: {example_input}\n")
print("Retrieval-Based Chatbot:", retrieval_reply)
print("Generative Chatbot   :", generative_reply)
print("===================================")